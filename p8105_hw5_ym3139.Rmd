---
title: "p8105_hw5_ym3139"
author: 'Yutong Mao (UNI: ym3139)'
date: "2025-11-13"
output: github_document
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(broom)      # for tidy t-test output
```

# Problem 0
For this assignment, I created a public GitHub repository named `p8105_hw5_ym3139` along with an R project of the same name. All data files required for the assignment are stored in the `data/` subdirectory and accessed via relative paths.
All code for Problems 1 through 3 is contained within a single R Markdown file, `p8105_hw5_ym3139.Rmd`, which is rendered in `github_document` format.

# Problem 1
In this study, I employed a simulation approach to investigate the birthday paradox.
For groups ranging from 2 to 50 individuals, I estimated the probability that at least two members share the same birthday through 10,000 simulation runs.

```{r problem1-function}
# Function to simulate birthdays for a group and check for duplicates
check_birthday_match <- function(n) {
  
  # Simulate n birthdays as integers from 1 to 365
  birthdays <- sample(1:365, size = n, replace = TRUE)
  
  # Check whether there is at least one duplicated birthday
  any_duplicated <- any(duplicated(birthdays))
  
  # Return TRUE if there is a shared birthday, FALSE otherwise
  any_duplicated
}
```

```{r problem1-function-check}
check_birthday_match(2)    # very small group, low chance of match
check_birthday_match(50)   # larger group, higher chance of match
```

```{r problem1-simulation}
set.seed(1)  # set seed for reproducibility

# Create a data frame of all simulation settings
bday_sim_results <- 
  expand_grid(
    group_size = 2:50,      # number of people in the room
    iter       = 1:10000    # simulation iteration index
  ) |>
  mutate(
    # For each row, run the birthday function once
    has_match = map_lgl(group_size, check_birthday_match)
  ) |>
  group_by(group_size) |>
  summarize(
    # Estimated probability of at least one shared birthday
    prob_match = mean(has_match)
  )

# Look at the first few rows of the results
bday_sim_results
```


```{r problem1-plot}
# Plot the estimated probability as a function of group size
bday_sim_results |>
  ggplot(aes(x = group_size, y = prob_match)) +
  geom_line() +
  geom_point() +
  labs(
    x = "Group size (number of people)",
    y = "Estimated probability of shared birthday",
    title = "Birthday problem: simulation-based estimates"
  )
```

As shown in the chart, the probability of at least two people sharing a birthday increases rapidly as the group size expands. When fewer than 10 people are in the room, this probability approaches 0; when the group size reaches just over 20 people, the probability is approximately 0.5; and when the number of people reaches around 50, the probability approaches 1. This suggests that our intuition about the birthday problem may underestimate how quickly the probability increases with group size.


# Problem 2

In this study, I employed simulation methods to examine the test power of the one-sample t-test under different conditions of true population means. The sample size was fixed at \(n = 30\), and the standard deviation was fixed at \(\sigma = 5\).

```{r problem2-function}
# Function to simulate one dataset and run a one-sample t-test
sim_one_ttest <- function(mu, n = 30, sigma = 5) {
  
  # Generate n observations from Normal(mu, sigma)
  x <- rnorm(n, mean = mu, sd = sigma)
  
  # Perform one-sample t-test for H0: mu = 0
  t_out <- t.test(x, mu = 0)
  
  # Use broom::tidy to extract estimate and p-value
  t_tidy <- broom::tidy(t_out)
  
  # Return a tibble with the sample mean and p-value
  tibble(
    mu_hat  = t_tidy$estimate,   # sample mean
    p_value = t_tidy$p.value     # p-value for H0: mu = 0
  )
}
```

```{r problem2-function-check}
sim_one_ttest(mu = 0)
sim_one_ttest(mu = 3)
```
```{r problem2-simulation}
set.seed(1)  # set seed for reproducibility

# Run 5000 simulations for each true value of mu
ttest_results <- 
  expand_grid(
    true_mu = 0:6,          # true mean values
    iter    = 1:5000        # simulation iteration index
  ) |>
  mutate(
    # For each row, simulate one dataset and run the t-test
    sim_df = map(true_mu, sim_one_ttest)
  ) |>
  unnest(sim_df) |>
  mutate(
    # Indicator for rejecting the null at alpha = 0.05
    reject_null = p_value < 0.05
  )

# Take a quick look at the simulated results
ttest_results |> head()

```
```{r problem2-power}
# Compute the power (proportion of rejections) for each true mu
power_results <- 
  ttest_results |>
  group_by(true_mu) |>
  summarize(
    power = mean(reject_null)   # proportion of times H0 is rejected
  )

power_results
```
```{r problem2-power-plot}
# Plot power (proportion of rejections) vs true mu
power_results |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True value of mu",
    y = "Power (proportion of rejections)",
    title = "Power of one-sample t-test as a function of the true mean"
  )
```

The figure shows that the power of the one-sample t-test gradually increases as the true mean moves away from zero. When the true mean is zero, the rejection probability approaches the nominal Type I error rate of 0.05. As the true mean increases from 1 to 6, the test power steadily rises and approaches 1.
This indicates a positive correlation between effect size and test power: the larger the effect, the easier it is to detect.

As the true mean moves farther from 0 (i.e., the effect size increases), the test's power rises; when μ=0, the rejection rate approaches 0.05, but as μ increases, power rapidly approaches 1.

```{r problem2-mu-estimates}
# Compute average estimates of mu_hat
mu_estimates <- 
  ttest_results |>
  group_by(true_mu) |>
  summarize(
    avg_mu_hat_all    = mean(mu_hat),                # mean over all samples
    avg_mu_hat_reject = mean(mu_hat[reject_null])    # mean only when H0 is rejected
  )

mu_estimates
```

```{r problem2-mu-estimates-long}
# Reshape to long format for plotting
mu_estimates_long <- 
  mu_estimates |>
  pivot_longer(
    avg_mu_hat_all:avg_mu_hat_reject,
    names_to  = "estimate_type",
    values_to = "avg_estimate"
  ) |>
  mutate(
    # Give more descriptive labels for plotting
    estimate_type = recode(
      estimate_type,
      avg_mu_hat_all    = "All samples",
      avg_mu_hat_reject = "Samples with rejection"
    )
  )

mu_estimates_long
```

```{r problem2-mu-estimates-plot}
# Plot average mu_hat vs true mu for all samples and rejected samples
mu_estimates_long |>
  ggplot(aes(x = true_mu, y = avg_estimate, color = estimate_type)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True value of mu",
    y = "Average of mu_hat",
    color = "Estimate type",
    title = "Average estimates of mu across all samples and rejected samples"
  )
```

Across all samples, the average of μ̂ approximates the true value \(\mu\) under each setting, consistent with the property of sample means as unbiased estimators.

However, when averaging μ̂ only from samples that reject the null hypothesis, the resulting mean exceeds the true value \(\mu\). This indicates selection bias: we condition only on events where the observed mean is far from zero and yields a small p-value.
Therefore, in hypothesis rejection tests, the sample mean of μ̂ does not approximate the true value \(\mu\).


# Problem 3
In this study, I utilized homicide data from 50 major U.S. cities to analyze the proportion of unsolved homicides in each city.

```{r problem3-read}
# Read in homicide data from the local data/ folder
homicide_df <- 
  read_csv("data/homicide-data.csv")

# Take a quick look at the raw data structure
homicide_df |> 
  glimpse()
```

The raw dataset contains one record per homicide case, with variables describing the city, state, year, geographic location, victim demographics, and case disposition (e.g., “closed with arrest,” “unresolved/no arrest”). The data covers over 50,000 homicides across 50 major U.S. cities.

```{r problem3-city-summary}
# Create city_state and summarize total and unsolved homicides per city
city_summary <- 
  homicide_df |>
  mutate(
    # Combine city and state into a single variable
    city_state = str_c(city, ", ", state),
    # Indicator for unsolved cases
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |>
  group_by(city_state) |>
  summarize(
    total_homicides   = n(),                  # total cases in the city
    unsolved_homicides = sum(unsolved)        # number of unsolved cases
  ) |>
  ungroup()

city_summary |> 
  head()
```

For each city, I created a `city_state` variable (e.g., “Baltimore, Maryland”) and aggregated the total number of homicides and unsolved homicides for that city. ‘Unsolved’ includes cases classified as “closed without arrest” or “active/no arrest.”

```{r problem3-baltimore}
# Filter to Baltimore, MD and extract counts
baltimore_counts <- 
  city_summary |>
  filter(city_state == "Baltimore, MD")

# Run prop.test for the proportion of unsolved homicides
baltimore_test <- 
  prop.test(
    x = baltimore_counts$unsolved_homicides,
    n = baltimore_counts$total_homicides
  )

# Tidy the prop.test output
baltimore_tidy <- 
  broom::tidy(baltimore_test)

baltimore_tidy

# Pull the estimated proportion and confidence interval
baltimore_estimate <- baltimore_tidy |> pull(estimate)
baltimore_ci_low   <- baltimore_tidy |> pull(conf.low)
baltimore_ci_high  <- baltimore_tidy |> pull(conf.high)

baltimore_estimate
baltimore_ci_low
baltimore_ci_high
```

For Baltimore, MD, the estimated proportion of homicides that are unsolved is
`r round(baltimore_estimate, 3)`, with a 95% confidence interval from 
`r round(baltimore_ci_low, 3)` to `r round(baltimore_ci_high, 3)`.

```{r problem3-city-prop}
# Run prop.test for each city using map2 and list columns
city_prop_results <- 
  city_summary |>
  mutate(
    # List column with prop.test objects for each city
    prop_test = map2(
      unsolved_homicides, 
      total_homicides,
      ~ prop.test(x = .x, n = .y)
    ),
    # Tidy each prop.test result
    prop_tidy = map(prop_test, broom::tidy)
  ) |>
  # Keep only the city_state and the tidied results
  select(city_state, prop_tidy) |>
  unnest(prop_tidy) |>
  # Rename key columns for clarity
  rename(
    prop_unsolved = estimate,
    ci_low        = conf.low,
    ci_high       = conf.high
  )

city_prop_results |> 
  head()

```

Using a clean pipeline with list-comprehensions and the `map2` function, I applied the `prop.test` function to each city to estimate the proportion of unsolved homicides and its 95% confidence interval.

```{r problem3-plot, fig.height=8}
# Reorder cities by estimated proportion of unsolved homicides
city_prop_results_ordered <- 
  city_prop_results |>
  mutate(
    city_state = fct_reorder(city_state, prop_unsolved)
  )

# Plot estimated proportions and confidence intervals by city
city_prop_results_ordered |>
  ggplot(aes(x = city_state, y = prop_unsolved)) +
  geom_point() +
  geom_errorbar(
    aes(ymin = ci_low, ymax = ci_high),
    width = 0
  ) +
  coord_flip() +
  labs(
    x = "City",
    y = "Estimated proportion of unsolved homicides",
    title = "Proportion of unsolved homicides by city",
    caption = "95% confidence intervals from prop.test"
  )
```

Cities are ranked by their proportion of unsolved homicides. Some cities (such as those in the top region of the chart) exhibit relatively low proportions of unsolved homicides with narrower confidence intervals, indicating more stable clearance rates. Other cities show significantly higher proportions of unsolved homicides, with point estimates exceeding 0.6 and wider confidence intervals.

